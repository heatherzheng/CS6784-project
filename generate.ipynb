{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce9e8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "\n",
    "start = \"<|startoftext|> \"\n",
    "sep = \" <|sep|>\"\n",
    "\n",
    "\n",
    "\n",
    "def dict2obj(d):\n",
    "    \"\"\"Convert a dictionary to a class\"\"\"\n",
    "    if isinstance(d, list):\n",
    "        d = [dict2obj(x) for x in d]\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "\n",
    "    class Class:\n",
    "        pass\n",
    "\n",
    "    obj = Class()\n",
    "    for k in d:\n",
    "        obj.__dict__[k] = dict2obj(d[k])\n",
    "    return obj\n",
    "\n",
    "\n",
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0 or 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH\n",
    "    return length\n",
    "\n",
    "\n",
    "def generate(args, tokenizer, model, prompt):\n",
    "    args.length = adjust_length_to_model(\n",
    "        args.length, max_sequence_length=model.config.max_position_embeddings\n",
    "    )\n",
    "    prompt_text = start + prompt.strip() + sep\n",
    "    encoded_prompt = tokenizer.encode(\n",
    "        prompt_text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_prompt = encoded_prompt.to(args.device)\n",
    "\n",
    "    input_ids = None if encoded_prompt.size()[-1] == 0 else encoded_prompt\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=args.length + len(encoded_prompt[0]),\n",
    "        temperature=args.temperature,\n",
    "        top_k=args.k,\n",
    "        top_p=args.p,\n",
    "        repetition_penalty=args.repetition_penalty,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    # Remove the batch dimension when returning multiple sequences\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        # print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (\n",
    "            prompt_text\n",
    "            + text[\n",
    "                len(\n",
    "                    tokenizer.decode(\n",
    "                        encoded_prompt[0], clean_up_tokenization_spaces=True\n",
    "                    )\n",
    "                ) :\n",
    "            ]\n",
    "        )\n",
    "        generated_sequences.append(total_sequence)\n",
    "        # print(total_sequence)\n",
    "    return generated_sequences[0]\n",
    "\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set max generation length\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "# Define model class\n",
    "MODEL_CLASSES = {\"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer)}\n",
    "\n",
    "# Generation arguments\n",
    "args = collections.defaultdict(\n",
    "    model_type=\"gpt2\",\n",
    "    model_name_or_path='model_ml_full/checkpoint-80000',\n",
    "    prompt=\"\",\n",
    "    length=512,\n",
    "    stop_token=\"<|endoftext|>\",\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    k=0,\n",
    "    p=0.97,  # use nucleus sampling\n",
    "    seed=42,\n",
    "    no_cuda=False,\n",
    "    num_return_sequences=1,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    n_gpu=torch.cuda.device_count(),\n",
    ")\n",
    "\n",
    "# Convert dict to object\n",
    "args = dict2obj(args)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Load tokenizer and model\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
    "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config_class,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "model = model_class.from_pretrained(args.model_name_or_path)\n",
    "model.to(args.device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1201b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no pad token\n",
      "pad token added!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "# Add special tokens\n",
    "tokenizer.add_special_tokens({\"sep_token\": sep})\n",
    "tokenizer.add_special_tokens({\"bos_token\": start})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"no pad token\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"pad token added!\")\n",
    "    \n",
    "tokenizer.save_pretrained('model_ml_full/checkpoint-80000')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('model_ml_full/checkpoint-80000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc438107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7500 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 1/7500 [00:02<5:46:47,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 2/7500 [00:04<4:30:54,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 3/7500 [00:06<4:48:00,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 4/7500 [00:07<3:41:04,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 5/7500 [00:09<3:40:34,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 6/7500 [00:11<3:24:06,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 7/7500 [00:11<2:45:45,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 8/7500 [00:14<3:36:32,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 9/7500 [00:15<3:27:48,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 10/7500 [00:17<3:20:08,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 11/7500 [00:19<3:44:07,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 12/7500 [00:20<3:11:47,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 13/7500 [00:22<3:26:50,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 14/7500 [00:23<3:09:58,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 15/7500 [00:25<3:08:58,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 16/7500 [00:27<3:33:46,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 17/7500 [00:28<3:16:59,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 18/7500 [00:28<2:31:44,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 19/7500 [00:30<2:44:54,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 20/7500 [00:32<2:49:28,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 21/7500 [00:33<2:49:01,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 22/7500 [00:34<2:25:41,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 23/7500 [00:35<2:35:41,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 24/7500 [00:36<2:27:45,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 25/7500 [00:37<2:19:43,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 26/7500 [00:40<3:33:21,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 27/7500 [00:41<3:16:19,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 28/7500 [00:43<3:10:43,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 29/7500 [00:43<2:30:06,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 30/7500 [00:45<3:02:35,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 31/7500 [00:47<2:57:28,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 32/7500 [00:47<2:23:24,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 33/7500 [00:48<2:19:50,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 34/7500 [00:49<2:17:48,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 35/7500 [00:51<2:56:43,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 36/7500 [00:53<2:51:29,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 37/7500 [00:54<2:51:06,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 38/7500 [00:56<3:01:13,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 39/7500 [00:57<3:05:07,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 40/7500 [00:59<3:05:37,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 41/7500 [01:00<3:02:25,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 42/7500 [01:01<2:48:25,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 43/7500 [01:03<2:56:28,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 44/7500 [01:05<3:16:50,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 45/7500 [01:06<2:54:25,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "end = \"<|endoftext|>\"\n",
    "# Generate\n",
    "path = \"test_classifier/ml/\"\n",
    "count = 0\n",
    "with open(os.path.join(path,\"all_titles.txt\"),\"r\") as titles:   \n",
    "    titlelines = titles.readlines() \n",
    "    with open(os.path.join(path,\"generated_7500_0.9.txt\"),\"w+\") as gen:\n",
    "        for i in tqdm(range(7500)):\n",
    "            title = titlelines[i]\n",
    "            generated = generate(args, tokenizer, model, title)\n",
    "            gen.write(f\"{generated}{end}\\n\")\n",
    "            count += 1 \n",
    "    gen.close()\n",
    "titles.close()\n",
    "\n",
    "print(f\"{count} abstracts generation done \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f8506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ad4646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "end = \"<|endoftext|>\"\n",
    "generated = generate(args,tokenizer,model,\"BERTSCORE: EVALUATING TEXT GENERATION WITH BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b27cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> BERTSCORE: EVALUATING TEXT GENERATION WITH BERT <|sep|>   We introduce BERT, a transformer-based transformer-based text generation architecture that is able to output sentences with the intention of generating balanced natural language utterances, almost always using single or both evaluated cases. BERT is a powerful yet automated transformer-based text generator based on BERT and has large community compared to ELMoGenerator and ELMoPlain R-models. Furthermore, including a reinforced version of BERT in a two-stage model consists of a supplementary processing block for detecting and handling missing clauses that only have metadata about the original corpus. This further helps to justify replacing references and redundant sentences in the considered sentence generation stage. We find that by combining the pre-processing and improving the enhancement network, BERT's output over ELMoGenerator is able to consistently improve the quality of natural-language extraction systems like AccuritionVerse. \n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"test_classifier/hep/\"\n",
    "count=0\n",
    "end = \"<|endoftext|>\"\n",
    "\n",
    "\n",
    "with open(os.path.join(path,\"generated_1000.txt\"),\"r\") as gen:   \n",
    "    lines = gen.readlines() \n",
    "    with open(os.path.join(path,\"generated_1000_new.txt\"),\"w\") as new:\n",
    "        for line in lines:\n",
    "            count+=1\n",
    "            new.write(line.strip()+\"<|endoftext|>\"+'\\n')\n",
    "    new.close()\n",
    "gen.close()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\"test_classifier/ml/generated_7500_0.97.txt\",\"r\") as all:\n",
    "    lines=all.readlines()\n",
    "    print(len(lines))\n",
    "all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01603c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
